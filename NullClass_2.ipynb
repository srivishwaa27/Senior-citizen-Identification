{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc46502-101d-4a2a-97fa-582faba0b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45847c93-c044-4d77-9cca-6465737e438a",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5082c0-c988-4e37-8526-0683f732d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Dataset directory\n",
    "dataset_path = r'D:\\Gender Age Detector\\utkface_gender_split'\n",
    "IMG_SIZE = 128\n",
    "\n",
    "images = []\n",
    "ages = []\n",
    "genders = []\n",
    "\n",
    "# Go through each gender folder\n",
    "for gender_label, gender_name in enumerate(['male', 'female']):\n",
    "    gender_folder = os.path.join(dataset_path, gender_name)\n",
    "    for filename in os.listdir(gender_folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            try:\n",
    "                # Extract age from filename (format: age_gender_otherinfo.jpg)\n",
    "                age = int(filename.split('_')[0])\n",
    "\n",
    "                # Load and preprocess image\n",
    "                img_path = os.path.join(gender_folder, filename)\n",
    "                img = load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "                img_array = img_to_array(img) / 255.0  # Normalize\n",
    "\n",
    "                images.append(img_array)\n",
    "                ages.append(age)\n",
    "                genders.append(gender_label)  # 0 for male, 1 for female\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping file {filename} due to error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20757a4a-e9a2-4ddc-81f5-67bf23a9fd0b",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbfbf83f-6c84-4c53-a169-fdfa5c49436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images, dtype=\"float32\")\n",
    "ages = np.array(ages, dtype=\"int32\")\n",
    "genders = np.array(genders, dtype=\"int32\")\n",
    "genders = to_categorical(genders, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73516f5-4bf7-481e-8079-f1188805887b",
   "metadata": {},
   "source": [
    "# Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ecf36a-cae4-40bf-8f03-aaf863277aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split for both age and gender\n",
    "X_train, X_test, age_train, age_test, gender_train, gender_test = train_test_split(\n",
    "    images, ages, genders, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c464c5f-fe26-4360-8b2c-4ab53c1e3653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18967 images belonging to 2 classes.\n",
      "Found 4741 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    directory=dataset_path,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    directory=dataset_path,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a08d32-a9a6-41b2-8e29-58387ef7ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class AgeGenderDataGenerator(Sequence):\n",
    "    def __init__(self, folder_path, batch_size=32, img_size=128, shuffle=True):\n",
    "        self.folder_path = folder_path\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.shuffle = shuffle\n",
    "        self.image_paths = []\n",
    "        self.ages = []\n",
    "        self.genders = []\n",
    "\n",
    "        # Load image paths and extract labels\n",
    "        for gender_label, gender_name in enumerate(['male', 'female']):\n",
    "            gender_folder = os.path.join(folder_path, gender_name)\n",
    "            for filename in os.listdir(gender_folder):\n",
    "                if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                    try:\n",
    "                        age = int(filename.split('_')[0])\n",
    "                        img_path = os.path.join(gender_folder, filename)\n",
    "\n",
    "                        self.image_paths.append(img_path)\n",
    "                        self.ages.append(age)\n",
    "                        self.genders.append(gender_label)\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "        self.indexes = np.arange(len(self.image_paths))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        batch_images = []\n",
    "        batch_ages = []\n",
    "        batch_genders = []\n",
    "\n",
    "        for i in batch_indexes:\n",
    "            img = cv2.imread(self.image_paths[i])\n",
    "            img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "            img = img.astype('float32') / 255.0\n",
    "\n",
    "            batch_images.append(img)\n",
    "            batch_ages.append(self.ages[i])\n",
    "            batch_genders.append(self.genders[i])\n",
    "\n",
    "        batch_images = np.array(batch_images)\n",
    "        batch_ages = np.array(batch_ages)\n",
    "        batch_genders = np.array(batch_genders)\n",
    "\n",
    "        # One-hot encode gender\n",
    "        batch_genders = np.eye(2)[batch_genders]\n",
    "\n",
    "        return batch_images, {'age_output': batch_ages, 'gender_output': batch_genders}\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc078ca-f846-42d1-9857-2e588f328fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(128, 128, 3))\n",
    "\n",
    "# Shared CNN base\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Age prediction branch (regression)\n",
    "age_output = Dense(1, name='age_output')(x)\n",
    "\n",
    "# Gender prediction branch (classification)\n",
    "gender_output = Dense(2, activation='softmax', name='gender_output')(x)\n",
    "\n",
    "# Define model\n",
    "model = Model(inputs=input_layer, outputs=[age_output, gender_output])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss={'age_output': 'mse', 'gender_output': 'categorical_crossentropy'},\n",
    "              metrics={'age_output': 'mae', 'gender_output': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d663a157-ce3f-4bb9-8b03-6603447525a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "741/741 [==============================] - 479s 645ms/step - loss: 349.7587 - age_output_loss: 348.9314 - gender_output_loss: 0.8273 - age_output_mae: 14.2127 - gender_output_accuracy: 0.6709 - val_loss: 243.4281 - val_age_output_loss: 242.9410 - val_gender_output_loss: 0.4873 - val_age_output_mae: 11.4145 - val_gender_output_accuracy: 0.8246\n",
      "Epoch 2/10\n",
      "741/741 [==============================] - 464s 626ms/step - loss: 216.4304 - age_output_loss: 215.7321 - gender_output_loss: 0.6982 - age_output_mae: 11.2889 - gender_output_accuracy: 0.7699 - val_loss: 182.9880 - val_age_output_loss: 182.6023 - val_gender_output_loss: 0.3856 - val_age_output_mae: 10.0984 - val_gender_output_accuracy: 0.8524\n",
      "Epoch 3/10\n",
      "741/741 [==============================] - 480s 648ms/step - loss: 178.6194 - age_output_loss: 177.9610 - gender_output_loss: 0.6583 - age_output_mae: 10.2479 - gender_output_accuracy: 0.7926 - val_loss: 183.4068 - val_age_output_loss: 183.0197 - val_gender_output_loss: 0.3870 - val_age_output_mae: 10.9746 - val_gender_output_accuracy: 0.8574\n",
      "Epoch 4/10\n",
      "741/741 [==============================] - 506s 683ms/step - loss: 154.0574 - age_output_loss: 153.4865 - gender_output_loss: 0.5708 - age_output_mae: 9.5263 - gender_output_accuracy: 0.8142 - val_loss: 172.9485 - val_age_output_loss: 172.6319 - val_gender_output_loss: 0.3166 - val_age_output_mae: 9.6359 - val_gender_output_accuracy: 0.8676\n",
      "Epoch 5/10\n",
      "741/741 [==============================] - 483s 652ms/step - loss: 137.6198 - age_output_loss: 137.1297 - gender_output_loss: 0.4901 - age_output_mae: 8.9461 - gender_output_accuracy: 0.8298 - val_loss: 126.5121 - val_age_output_loss: 126.1661 - val_gender_output_loss: 0.3459 - val_age_output_mae: 8.3894 - val_gender_output_accuracy: 0.8638\n",
      "Epoch 6/10\n",
      "741/741 [==============================] - 473s 638ms/step - loss: 124.3722 - age_output_loss: 123.8780 - gender_output_loss: 0.4942 - age_output_mae: 8.4966 - gender_output_accuracy: 0.8313 - val_loss: 108.3116 - val_age_output_loss: 108.0395 - val_gender_output_loss: 0.2721 - val_age_output_mae: 7.7611 - val_gender_output_accuracy: 0.8940\n",
      "Epoch 7/10\n",
      "741/741 [==============================] - 487s 657ms/step - loss: 114.6585 - age_output_loss: 114.2138 - gender_output_loss: 0.4446 - age_output_mae: 8.1726 - gender_output_accuracy: 0.8468 - val_loss: 94.9916 - val_age_output_loss: 94.7353 - val_gender_output_loss: 0.2562 - val_age_output_mae: 7.3861 - val_gender_output_accuracy: 0.8992\n",
      "Epoch 8/10\n",
      "741/741 [==============================] - 498s 673ms/step - loss: 109.8784 - age_output_loss: 109.4592 - gender_output_loss: 0.4194 - age_output_mae: 7.9831 - gender_output_accuracy: 0.8558 - val_loss: 91.7436 - val_age_output_loss: 91.4103 - val_gender_output_loss: 0.3331 - val_age_output_mae: 7.3180 - val_gender_output_accuracy: 0.8783\n",
      "Epoch 9/10\n",
      "741/741 [==============================] - 495s 668ms/step - loss: 103.2942 - age_output_loss: 102.8777 - gender_output_loss: 0.4166 - age_output_mae: 7.7617 - gender_output_accuracy: 0.8574 - val_loss: 84.7697 - val_age_output_loss: 84.5142 - val_gender_output_loss: 0.2555 - val_age_output_mae: 6.9593 - val_gender_output_accuracy: 0.9046\n",
      "Epoch 10/10\n",
      "741/741 [==============================] - 486s 656ms/step - loss: 101.0921 - age_output_loss: 100.6549 - gender_output_loss: 0.4372 - age_output_mae: 7.6704 - gender_output_accuracy: 0.8564 - val_loss: 87.1893 - val_age_output_loss: 86.9618 - val_gender_output_loss: 0.2275 - val_age_output_mae: 6.9763 - val_gender_output_accuracy: 0.9112\n"
     ]
    }
   ],
   "source": [
    "# Create training and validation generators\n",
    "dataset_path = r'D:\\Gender Age Detector\\utkface_gender_split'\n",
    "train_gen = AgeGenderDataGenerator(dataset_path, batch_size=32, img_size=128, shuffle=True)\n",
    "val_gen = AgeGenderDataGenerator(dataset_path, batch_size=32, img_size=128, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    validation_data=val_gen,\n",
    "                    epochs=10,\n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febf2bb5-b6d8-4028-852f-dfaf1f4209d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 79s 106ms/step - loss: 87.1893 - age_output_loss: 86.9618 - gender_output_loss: 0.2275 - age_output_mae: 6.9763 - gender_output_accuracy: 0.9112\n",
      "loss: 87.18933868408203\n",
      "age_output_loss: 86.96175384521484\n",
      "gender_output_loss: 0.227542445063591\n",
      "age_output_mae: 6.97626256942749\n",
      "gender_output_accuracy: 0.9112114310264587\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation generator\n",
    "results = model.evaluate(val_gen)\n",
    "\n",
    "# Print each metric name and its value\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(f\"{name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de0c8210-b5f9-4d18-a26c-2bb7713be301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VISHWA\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save('age_gender_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf236ec-527f-4a70-8d4e-b916c26b4750",
   "metadata": {},
   "source": [
    "# Webcam Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6337b336-6f80-4690-9ff7-8a4233badc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "Detected: Female, Age: 37 at 2025-07-25 21:42:11\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Detected: Male, Age: 24 at 2025-07-25 21:42:12\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Detected: Male, Age: 21 at 2025-07-25 21:42:12\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Detected: Male, Age: 28 at 2025-07-25 21:42:15\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Detected: Male, Age: 35 at 2025-07-25 21:42:15\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Detected: Male, Age: 31 at 2025-07-25 21:42:16\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Detected: Female, Age: 31 at 2025-07-25 21:42:19\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Store last seen face positions and timestamp\n",
    "last_faces = []\n",
    "last_logged_time = {}\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect faces\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    face_imgs = []\n",
    "    face_coords = []\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_id = f\"{x}_{y}_{w}_{h}\"\n",
    "        recent = False\n",
    "\n",
    "        for px, py, pw, ph in last_faces:\n",
    "            if abs(x - px) < 20 and abs(y - py) < 20:\n",
    "                recent = True\n",
    "                break\n",
    "\n",
    "        if recent:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            face = frame[y:y+h, x:x+w]\n",
    "            face = cv2.resize(face, (128, 128))\n",
    "            face = face.astype('float32') / 255.0\n",
    "            face_imgs.append(face)\n",
    "            face_coords.append((x, y, w, h))\n",
    "            last_faces.append((x, y, w, h))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if len(face_imgs) > 0:\n",
    "        face_imgs = np.array(face_imgs)\n",
    "        predicted_ages, predicted_genders = model.predict(face_imgs)\n",
    "        predicted_gender_labels = [gender_labels[np.argmax(g)] for g in predicted_genders]\n",
    "\n",
    "        for i, (x, y, w, h) in enumerate(face_coords):\n",
    "            age = int(predicted_ages[i])\n",
    "            gender = predicted_gender_labels[i]\n",
    "            label = f\"{gender}, Age: {age}\"\n",
    "            if age >= 60:\n",
    "                label += \" (Senior)\"\n",
    "\n",
    "            # Draw\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "            # Log once every 10 seconds\n",
    "            face_key = f\"{x}_{y}_{w}_{h}\"\n",
    "            current_time = time.time()\n",
    "            if face_key not in last_logged_time or (current_time - last_logged_time[face_key]) > 10:\n",
    "                last_logged_time[face_key] = current_time\n",
    "                time_of_visit = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                # Save to CSV\n",
    "                new_entry = pd.DataFrame([[age, gender, time_of_visit]], columns=['Age', 'Gender', 'Time'])\n",
    "                new_entry.to_csv(log_file, mode='a', header=False, index=False)\n",
    "\n",
    "                # Print the result\n",
    "                print(f\"Detected: {gender}, Age: {age} at {time_of_visit}\" + (\" (Senior)\" if age >= 60 else \"\"))\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Senior Citizen Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
